\chapter{Basics of Recursive Bayesian Estimation}

In following sections the problem of recursive Bayesian estimation is stated and its analytical
solution is derived. Later on, due to practical intractability of the solution in its general form,
a few methods that either simplify the problem or approximate the solution are shown.

\section{Problem Statement}

Assume a dynamic system described by a hidden real-valued \emph{state vector} \(x\) which evolves at
discrete time steps according to a known function \(f_t\) (in this text called \emph{process model})
as described by \eqref{eq:DynSysFt}.
\begin{equation} \label{eq:DynSysFt}
	x_t = f_t(x_{t-1}, v_{t-1})
\end{equation}

Variable \(v_t\) in \eqref{eq:DynSysFt} denotes random \emph{process noise}, which may come from various
sources and is often inevitable. Sequence of \(v_t\) is assumed to be identically independently
distributed random variable sequence.

The state of the system is hidden and can only be observed though a real-valued \emph{observation vector}
\(y\) that relates to the state \(x\) as in \eqref{eq:DynSysHt}, but adds further \emph{observation
noise} \(w\).
\begin{equation} \label{eq:DynSysHt}
	y_t = h_t(x_t, w_t)
\end{equation}

In \eqref{eq:DynSysHt} \(h_t\) is known function called \emph{observation model} in this text and \(w_t\) is
identically independently distributed random variable sequence that denotes observation noise.

The goal of recursive\footnote{by the word recursive we mean that it is not needed to keep track of
the whole batch of previous observations in practical methods, only appropriate quantities from time
moments \(t-1\) and \(t\) are needed to estimate \(x_t\). However, this does not apply to the
derivation of the solution, where the notation of whole batch of observations \(y_{1:t}\) is used.}
Bayesian estimation is to give an estimate of the state \(x_t\) given the
observations \(y_{1:t}\) provided the knowledge of the functions \(f_t\) and \(h_t\).
More formally, the goal is to find the {\pdf} \(p(x_t | y_{1:t})\).
Theoretical solution to this problem is known and is presented in next section.

\section{Theoretical solution}

At first, we observe that {\pdf} \(p(x_t|x_{t-1})\) can be derived from the process model
\eqref{eq:DynSysFt} (given the distribution of \(v_k\)) and that \(p(y_t|x_t)\) can be derived from
the observation model \eqref{eq:DynSysHt} respectively. (given the distribution of \(w_k\))

Because recursive solution is requested, suppose that \(p(x_{t-1}|y_{1:t-1})\) and
\(p(x_0)\) are known\footnote{\(p(x_0)\) can be called initial {\pdf} of the state vector.} in
order to be able to make the transition \(t-1 \; \rightarrow \; t\).

In the first stage that can be called \emph{prediction}, \emph{a priori} {\pdf}
\(p(x_t | y_{1:t-1})\) is calculated without knowledge of \(y_t\). We begin the derivation by
performing the reverse of the marginalization over \(x_{k-1}\).
\begin{equation*}
	p(x_t | y_{1:t-1}) = \int_{-\infty}^{\infty} p(x_t, x_{t-1} | y_{1:t-1}) \; \dx_{t-1}
\end{equation*}

Using chain rule for {\pdfs}, the element of integration can be split.
\begin{equation*}
	p(x_t | y_{1:t-1}) = \int_{-\infty}^{\infty} p(x_t | x_{t-1}, y_{1:t-1}) p(x_{t-1} | y_{1:t-1}) \; \dx_{t-1}
\end{equation*}

With an assumption that the modelled dynamic system \eqref{eq:DynSysFt} possesses \emph{Markov
Property}\footnote{an assumption of independence that states that system state in time \(t\) only
depends on system state in \(t-1\) (and is not directly affected by previous states).},
\(p(x_t | x_{t-1}, y_{1:t-1})\) equals \(p(x_t | x_{t-1})\).~\cite{AruMasGor:02}
This leaves us with the result \eqref{eq:APrioriPdf}.
\begin{equation} \label{eq:APrioriPdf}
	p(x_t | y_{1:t-1}) = \int_{-\infty}^{\infty} p(x_t | x_{t-1}) p(x_{t-1} | y_{1:t-1}) \; \dx_{t-1}
\end{equation}

As we can see, a priori {\pdf} only depends on previously known functions and therefore can be
calculated.

We continue with the second stage that could be named \emph{update}, where new observation \(y_t\) is taken into
account and \emph{a posteriori} {\pdf} \(p(x_t | y_{1:t})\) is calculated. Bayes' theorem can be used
to derive a posteriori {\pdf} \eqref{eq:APosterioriPdfRaw}.
\begin{equation} \label{eq:APosterioriPdfRaw}
	p(x_t | y_{1:t}) = \frac{p(y_t | x_t, y_{1:t-1}) p(x_t | y_{1:t-1})}{p(y_t | y_{1:t-1})}
\end{equation}

According to the observation model \eqref{eq:DynSysHt} and assuming Markov property, \(y_t\) only
depends on \(x_t\). That is \(p(y_t | x_t, y_{1:t-1}) = p(y_t | x_t)\). Therefore a posteriori
{\pdf} can be further simplified into \eqref{eq:APosterioriPdf}.
\begin{equation} \label{eq:APosterioriPdf}
	p(x_t | y_{1:t}) = \frac{p(y_t | x_t) p(x_t | y_{1:t-1})}{p(y_t | y_{1:t-1})}
\end{equation}

While both {\pdfs} in the numerator of \eqref{eq:APosterioriPdf} are already known, \(p(y_t|y_{1:t-1})\)
found in the denominator can be calculated using the formula \eqref{eq:MargLikelihood}, where
marginalization over \(x_t\) is preformed. Quantity \eqref{eq:MargLikelihood} can also be interpreted as
\emph{marginal likelihood} (sometimes called \emph{evidence}) of observation.~\cite{Smi:10}
\begin{equation} \label{eq:MargLikelihood}
	p(y_t | y_{1:t-1}) = \int_{-\infty}^{\infty} p(y_t | x_t) p(x_t | y_{1:t-1}) \; \dx_{t}
\end{equation}

Computing \eqref{eq:MargLikelihood} isn't however strictly needed as it does not depend on \(x_t\) and
serves as a normalising constant in \eqref{eq:APosterioriPdf}. Depending on use-case the normalising
constant may not be needed at all or may be computed alternatively using the fact that \(p(x_t | y_{1:y})\)
integrates to \(1\).

We have shown that so called \emph{optimal Bayesian solution}\cite{AruMasGor:02} can be easily
analytically inferred using only \emph{chain rule for {\pdfs}}, \emph{marginalization} and
\emph{Bayes' theorem}. (equations \eqref{eq:APrioriPdf}, \eqref{eq:APosterioriPdf} and
\eqref{eq:MargLikelihood} forming the main steps of the solution) On the other hand, using this
method directly in practice proves difficult because at least one parametric multidimensional
integration has to be performed (in \eqref{eq:APrioriPdf}), which is (in its general form) hardly
tractable for greater than small state vector dimensions.

This is a motivation for various simplifications and approximations among which we have chosen
Kalman filter described in the next section and particle filter family described later.

\section{Kalman Filter}

Kalman filter\footnote{first presented by Rudolf Emil Kalman in 1960.} poses additional set of strong
assumptions on modelled dynamic system, but greatly
simplifies optimal Bayesian solution \eqref{eq:APrioriPdf}, \eqref{eq:APosterioriPdf} into a
sequence of algebraic operations with matrices. On the other hand, when these requirements can be
fulfilled, there is no better estimator in Bayesian point of view because Kalman filter computes
\(p(x_t | y_{1:t})\) \emph{exactly.}\footnote{not accounting for numeric errors that arise in
practical implementations.}

Assumptions additionally posed on system by Kalman filter are:
\begin{enumerate}
	\item \(f_t\) in the process model \eqref{eq:DynSysFt} is a linear function of \(x_t\) and
	\(v_t\).
	\item \(v_t \sim \mathcal{N}(0, Q_t)\) meaning that process noise \(v_t\) is normally
	distributed with zero mean\footnote{zero mean assumption is not strictly needed, it is however
	common in many implementations.} and with known covariance matrix \(Q_t\).
	\item \(h_t\) in the observation model \eqref{eq:DynSysHt} is a linear function of \(x_t\) and
	\(w_t\).
	\item \(w_t \sim \mathcal{N}(0, R_t)\) meaning that observation noise \(w_t\) is normally distributed
	with zero mean and with known covariance matrix \(R_t\).
	\item initial state {\pdf} is Gaussian.
\end{enumerate}

It can be proved that if above assumptions hold, \(p(x_t|y_{1:t})\) is Gaussian for all
\(t > 0\).~\cite{Pet:81} Furthermore, given assumptions 1. and 2. the process model
\eqref{eq:DynSysFt} can be reformulated as \eqref{eq:LinSysAt}, where \(A_t\) is real-valued matrix
that represents \(f_t\).
Using the same idea and assumptions 3. and 4. the observation model \eqref{eq:DynSysHt} can be
expressed as \eqref{eq:LinSysCt}, \(C_t\) being real-valued matrix representing \(h_t\). Another
common requirement used below in algorithm description is that \(v_t\) and \(w_t\) are
stochastically independent.
\begin{align}
	x_t &= A_t x_{t-1} + \hat{v}_{t-1} & A_t &\in \mathbb{R}^{n,n} \;\; n \in \mathbb{N} \label{eq:LinSysAt} \\
	y_t &= C_t x_t + \hat{w}_t & C_t &\in \mathbb{R}^{j,n} \;\; j \in \mathbb{N} \;\; j \leq n \label{eq:LinSysCt}
\end{align}

Note that we have marked noises \(v_t\) and \(w_t\) as \(\hat{v}_t\) and \(\hat{w}_t\) when they
are transformed through \(A_t\), respectively \(C_t\) matrix. Let also \(\hat{Q}_t\) denote the
covariance matrix of \(\hat{v}_t\) and \(\hat{R}_t\) denote the covariance matrix of \(\hat{w}_t\)
in further text.

At this point we can describe the algorithm of Kalman filter. As stated above, a posteriori {\pdf}
is Gaussian and thus can be parametrised by mean vector \(\mu\) and covariance matrix \(P\). Let us
denote a posteriori mean from previous iteration by \(\mu_{t-1|t-1}\) and associated covariance by
\(P_{t-1|t-1}\) as in \eqref{eq:KalmanPreAPost}.
\begin{equation} \label{eq:KalmanPreAPost}
	p(x_{t-1} | y_{1:t-1}) = \mathcal{N}(\mu_{t-1|t-1}, P_{t-1|t-1})
\end{equation}

A priori {\pdf} \eqref{eq:KalmanAPrior} can then be calculated as follows:~\cite{AruMasGor:02}
\begin{align}
	p(x_t | y_{1:t-1}) &= \mathcal{N}(\mu_{t|t-1}, P_{t|t-1}) \label{eq:KalmanAPrior} \\
	\mu_{t|t-1} &= A_t \mu_{t-1|t-1} \notag \\
	P_{t|t-1} &= A_t P_{t-1|t-1} A_t^T + \hat{Q}_{t-1} \notag
\end{align}

Before introducing a posteriori {\pdf} it is useful to establish another Gaussian {\pdf}
\eqref{eq:KalmanEvidence} that is not necessarily needed, but is useful because it represents
marginal likelihood \eqref{eq:MargLikelihood}. % TODO: citation for this!
\begin{align}
	p(y_t|y_{1:t-1}) &= \mathcal{N}(\nu_{t|t-1}, S_{t|t-1}) \label{eq:KalmanEvidence} \\
	\nu_{t|t-1} &= C_t \mu_{t|t-1} \notag \\
	S_{t|t-1} &= C_t P_{t|t-1} C_t^T + \hat{R}_t \notag
\end{align}

The update phase of Kalman filter can be performed by computing so-called \emph{Kalman gain} matrix
\eqref{eq:KalmanGain}, a posteriori {\pdf} \eqref{eq:KalmanAPost} is then derived from a priori one
using Kalman gain \(K_t\) and observation \(y_t\).~\cite{AruMasGor:02}
\begin{align}
	K_t &= P_{t|t-1} C_t^T S_{t|t-1}^{-1} \label{eq:KalmanGain} \\[\parskip]
	p(x_t|y_{1:t}) &= \mathcal{N}(\mu_{t|t}, P_{t|t}) \label{eq:KalmanAPost} \\
	\mu_{t|t} &= \mu_{t|t-1} + K_t(y_t - \nu_{t|t-1}) \notag \\
	P_{t|t} &= P_{t|t-1} - K_t C_t P_{t|t-1} \notag
\end{align}

In all formulas above \(A^T\) denotes a transpose of matrix \(A\) and \(A^{-1}\) denotes inverse
matrix to \(A\). As can be seen, formulas \eqref{eq:APrioriPdf} and \eqref{eq:APosterioriPdf} have
been reduced to tractable algebraic operations, computing inverse matrix\footnote{it can be shown
that \(S_{t|t-1}\) is positive definite given that \(C_t\) is full-ranked,
therefore the inverse in \eqref{eq:KalmanGain} exists.} being the most costly one.

It should be further noted that Kalman filter and described algorithm can be easily enhanced to
additionally cope with \emph{intervention} (or control) vector applied to the system, making it
suitable for the theory of decision-making. Numerous generalisations of Kalman filter exist, for
example \emph{extended Kalman filter} that relaxes the requirement of linear system by locally
approximating non-linear system with Taylor series. These are out of scope of this
text, but provide areas for subsequent consideration.

On the other hand, the assumption of Gaussian a posteriori {\pdf} cannot be easily overcome and for
systems that show out non-Gaussian distributions of state vector another approach have to be
taken.~\cite{AruMasGor:02} One such approach can be Monte Carlo-based \emph{particle filter}
presented in the next section.

\section{Particle Filter} \label{sec:ParticleFilter}

Particle filters represent approximate solution of the problem of recursive Bayesian estimation,
thus can be considered \emph{suboptimal} methods. Underlying algorithm described below is most
commonly named \emph{sequential importance sampling (SIS)}. The biggest advantage of particle filtering
is that requirements posed on modelled system are much weaker than those assumed by optimal methods
such as Kalman filter. Simple form of particle filter presented in this section (that assumes that
modelled system has Markov property) requires only knowledge of {\pdf} \(p(x_t|x_{t-1})\)
representing the process model and knowledge of \(p(y_t|x_t)\) representing the observation
model.\footnote{both {\pdfs} are generally time-varying and their knowledge for all \(t\) is needed,
but their representation (parametrised by conditioning variable) is frequently constant in time in
practical applications.}

Sequential importance sampling approximates posterior density by weighted empirical
{\pdf} \eqref{eq:PFAPost}.
\begin{align}
	p(x_t | y_{1:t}) \approx \sum_{i=1}^N \omega_t^{(i)} \delta(x_t - x_t^{(i)}) \label{eq:PFAPost} \\
	\forall i \in \mathbb{N} \;\; i \leq N: \omega_i \geq 0 \quad\quad \sum_{i=1}^N \omega_i = 1 \notag
\end{align}

In \eqref{eq:PFAPost} \(x_t^{(i)}\) denotes value of i-th \emph{particle}: possible state of the
system at time \(t\);
\(\omega_t^{(i)}\) signifies weight of i-th particle at time \(t\): scalar value proportional to
expected probability of the system being in state in small neighbourhood of \(x_t^{(i)}\);
\(N\) denotes total number of particles\footnote{\(N\) is assumed to be
arbitrary but fixed positive integer for our uses. Variants of particle filter exist that use
adaptive number of particles, these are not discussed here.}, a significant tunable parameter
of the filter.

Described particle filter is bootstrapped by sampling \(N\) random particles from initial {\pdf}
\(p(x_0)\). Let \(i \in \mathbb{N} \;\; i \leq N\), transition \(t-1 \; \rightarrow \; t\) can be
performed as follows:
\begin{enumerate}
	\item for each \(i\) compute \(x_t^{(i)}\) by random sampling from conditional {\pdf}
		\(p(x_t|x_{t-1})\) where \(x_{t-1}^{(i)}\) substitutes \(x_{t-1}\) in condition. This step
		can be interpreted as a simulation of possible system state developments.
	\item for each \(i\) compute weight \(\omega_t^{(i)}\) using \eqref{eq:PFWeightUpdate}
		by taking observation \(y_t\) into account. \(x_t\) is substituted by \(x_t^{(i)}\) in
		condition in \eqref{eq:PFWeightUpdate}. Simulated system states are confronted with reality
		through observation.
		\begin{equation} \label{eq:PFWeightUpdate}
			\omega_t^{(i)} = p(y_t | x_t) \omega_{t-1}^{(i)}
		\end{equation}
	\item normalise weights according to \eqref{eq:PFWeightNormalise} so that approximation of
		a~posteriori {\pdf} integrates to one.
		\begin{equation} \label{eq:PFWeightNormalise}
			\omega_t^{(i)} = \frac{\omega_t^{(i)}}{\sum_{j=1}^N \omega_t^{(j)}}
		\end{equation}
\end{enumerate}

Relative computational ease of described algorithm comes with
cost: first, particle filter is in principle non-deterministic because of the random sampling in
step~1, in other words, particle filter is essentially a Monte Carlo method; second, appropriate
number of particles \(N\) has to be chosen --- too small \(N\) can lead to significant approximation
error while inadequately large \(N\) can make particle filter infeasibly time-consuming. It can be
proved that particle filter converges to true a~posteriori density as \(N\) approaches
infinity and certain other assumptions hold~\cite{CriDou:02}, therefore the number of particles
should be chosen as a~balance of accuracy and speed.

Only two operations with {\pdfs} were needed: sampling from \(p(x_t|x_{t-1})\) and evaluating
\(p(y_t | x_t)\) in known point. Sometimes sampling from \(p(x_t|x_{t-1})\) is not
feasible\footnote{but can be replaced by evaluation in known point.} and/or better results are
expected by taking observation \(y_t\) into account during sampling (step~1). This can be
achieved by introducing so-called \emph{proposal density} (sometimes \emph{importance density})
\(q(x_t|x_{t-1}, y_t)\). Sampling in step~1 then uses \(q(x_t|x_{t-1}, y_t)\) instead, where \(x_{t-1}\) in
condition is substituted by \(x_{t-1}^{(i)}\). Weight computation in step~2 have to be replaced with
\eqref{eq:PFWeightUpdateProp} that compensates different sampling distribution (every occurrence of
\(x_t\), \(x_{t-1}\) in mentioned formula has to be substituted by \(x_t^{(i)}\) and \(x_{t-1}^{(i)}\)
respectively). See \cite{AruMasGor:02} for derivation of these formulas and for discussion about
choosing adequate proposal density.
\begin{equation} \label{eq:PFWeightUpdateProp}
	\omega_t^{(i)} = \frac{p(y_t|x_t)p(x_t|x_{t-1})}{q(x_t|x_{t-1}, y_t)} \omega_{t-1}^{(i)}
\end{equation}

Particle filters also suffer from a phenomenon known as \emph{sample impoverishment} or
\emph{degeneracy problem}: after a few iterations all but one particles' weight falls close to
zero.\footnote{it has been shown that variance of particle weights continually raises as algorithm
progresses.~\cite{AruMasGor:02}} A measurement of degeneracy can be obtained by computing an
approximate of \emph{effective sample size} \(N_{\text{eff}}\) at given time \(t\) using
\eqref{eq:PFNeff}.~\cite{AruMasGor:02}
\begin{equation} \label{eq:PFNeff}
	N_{\text{eff}} \approx \left( \sum_{i=1}^N \left( \omega_t^{(i)} \right)^2 \right)^{-1}
\end{equation}

Very small \(N_{\text{eff}}\) compared to \(N\) signifies substantial loss of ``active'' particles,
which is certainly undesirable as it hurts accuracy while leaving computational demands unchanged.
One technique to diminish this is based on careful choice of proposal density (as explained in
\cite{AruMasGor:02}) second one is to add additional \emph{resample} step to above
algorithm:\footnote{this additional step may be performed in every iteration or just when
\(N_{\text{eff}}\) falls below certain threshold.}
\begin{enumerate}[resume] % so that enumeration starts with number 4
	\item for each \(i\) resample \(x_t^{(i)}\) from approximate a posteriori {\pdf}
		\(\sum_{i=1}^N \omega_t^{(i)} \delta(x_t - x_t^{(i)})\). Given that sampling is truly random
		and independent this means that each particle is in average copied \(n_i\) times, where \(n_i\)
		is roughly proportional to particle weight: \(n_i \approx \omega_t^{(i)} N\).
\end{enumerate}

Step~4 therefore facilitates avoidance of particles with negligible weight by replacing them with more weighted
ones. Such enhanced algorithm is known as \emph{sequential importance resampling (SIR)}.

Recursive Bayesian estimation using SIR methods can be applied to a wide range of dynamic systems
(even to those where more specialised methods fail) and can be tuned with number of particles \(N\) and
proposal density \(q\). On the other hand a method specially designed for a given system easily
outperforms general particle filter in terms of speed and accuracy.

\section{Marginalized Particle Filter}

TODO: MPF. If in lack of time, write short mention and merge into PF section. % TODO: MPF
