\chapter{Software analysis}

In this chapter general software development approaches and practices will be confronted with
requirements posed on the desired software library for recursive Bayesian estimation. After stating these
requirements, feasibility of various programming paradigms applied to our real-world problem is
discussed. Continues a comparison of suitable features of 3 chosen programming
languages: C++, MATLAB language and Python. Emphasis is put on the Python/Cython combination that was
chosen for implementation.

In whole chapter, the term \emph{user} refers to someone (a programmer) who uses the library in
order to implement higher-level functionality (such as simulation of dynamic systems).

\section{Requirements}

Our intended audience is a broad scientific community interested in the field of the recursive
Bayesian estimation and decision-making. Keeping this in mind and in order to formalise expectations for
the desired library for Bayesian filtering, the following set of requirements was developed.

\noindent Functionality:
\begin{itemize}
	\item Framework for working with potentially conditional {\pdfs} should be implemented
		including support for basic operations such as product and chain rule. The chain rule
		implementation should be flexible in a way that for example
		\(p(a_t,b_t|a_{t-1},b_{t-1}) = p(a_t|a_{t-1},b_t)p(b_t|b_{t-1})\) product can be
		represented.
	\item Basic Bayesian filtering methods such as the Kalman and particle filter have to be present,
		plus at least one of more specialised algorithms --- a marginalized particle filter or
		non-linear Kalman filter variants.
\end{itemize}
General:
\begin{itemize}
	\item Up-to-date, complete and readable API\footnote{Application Programming Interface, a set of
		rules that define how a particular library is used.} documentation is required. Such
		documentation should be well understandable by someone that already understands mathematical
		background of the particular algorithm.
	\item High level of interoperability is needed; data input/output should be straightforward as
		well as using existing solutions for accompanying tasks such as visualising the results.
	\item The library should be platform-neutral and have to run on major server and workstation
		platforms, at least on Microsoft Windows and GNU/Linux.
	\item The library should be Free/Open-source software as it is believed by the authors that such
		licensing/development model results in software of greatest quality in long term. Framework
		used by the library should make it easy to adapt and extend the library for various needs.
\end{itemize}
Usability:
\begin{itemize}
	\item Initial barriers for installing and setting up the library should be lowest possible.
		For example a necessity to install third-party libraries from sources is considered
		infeasible.
	\item Implementation environment used for the library should allow for high programmer
		productivity; prototyping new solutions should be quick and cheap (in terms of effort)
		operation. This requirement effectively biases towards higher-level programming
		languages.
\end{itemize}
Performance:
\begin{itemize}
	\item Computational overhead\footnote{excess computational costs not directly involved
		in solving particular problem; for example interpreter overhead.} should be kept reasonably
		low.
	\item Applications built atop of the library should be able to scale well on multi-processor
		systems. This can be achieved for example by thread-safety of critical library objects
		or by explicit parallelisation provided by the library.
\end{itemize}

It is evident that some of the requirements are antagonistic, most prominent example being demand
for \emph{low computational overhead} while still offering \emph{high programmer productivity} and
rapid prototyping. The task of finding tradeoffs between contradictory tendencies or developing smart
solutions that work around traditional limitations is left upon the implementations.

\section{Programming paradigms}

Many programming paradigms exist and each programming language usually suggests a particular paradigm,
though many languages let programmers choose from or combine multiple paradigms. This section
discusses how well could be three most prominent paradigms (procedural, object-oriented and
functional) applied to the software library for Bayesian filtering. Later on additional features of
implementation environments such as interpreted vs. compiled approach or argument passing convention
are evaluated.

\subsection{Procedural paradigm}

The procedural paradigm is the traditional approach that appeared along the first high-level programming languages.
The procedural programming can be viewed as a structured variant of imperative programming, where
programmer specifies steps (in form of orders) needed to reach desired program state.
Structured approach that emphasizes dividing the code into logical and self-contained
blocks (procedures, modules) is used to make the code more reusable, extensible and modular.
Today's most notable procedural languages include C and Fortran.

Most procedural languages are associated with very low overhead (performance of programs
compiled using optimising compiler tend to be very close to ideal programs written in
assembly code); mentioned languages are also spread and well-known in scientific computing.

On the other hand, while possible, it is considered an elaborate task by the author to write
a~modular and extensible library in these languages. Another disadvantage is that
usually only very basic building blocks are provided by the language --- structures like
lists and strings have to be supplied by the programmer or a third-party library. This only
adds to the fact that the procedural paradigm-oriented languages are commonly not easy to learn
and that programmer productivity associated with these languages may be much lower compared
to more high-level languages.

\subsection{Object-oriented paradigm} \label{sec:OOP}

The object-oriented paradigm extends the procedural approach with the idea of \emph{objects} ---
structures with procedures
(called \emph{methods}) and variables (called \emph{attributes}) bound to them. Other
feature frequently offered is \emph{polymorphism} (an extension to language's type
system that adds the notion of \emph{subtypes} and a rule that subtype of a given type can
be used everywhere where given type can be used) most often facilitated through a concept of
\emph{classes}, common models for sets of objects with same behaviour but different
payload; objects are then said to be \emph{instances} of classes. A subclass \emph{inherits}
methods and attributes from its superclass and can \emph{override} them or add its own.
\emph{Encapsulation}, a language mechanism to restrict access to certain object attributes and
methods, may be employed by the language to increase robustness by hiding implementation
details. In order to be considered object-oriented, statically typed languages
(p.~\pageref{desc:StaticTyping}) should provide \emph{dynamic dispatch}\footnote{a way of
calling methods where the exact method to call is resolved at runtime based on actual (dynamic)
object type (in contrast to static object type).}, en essential complement to polymorphism, for
certain or all object methods.

Notable examples of languages that support (although not exclusively) object-oriented
paradigm are statically typed C++, Java and dynamically typed (p.~\pageref{desc:DynamicTyping})
MATLAB language, Python, Smalltalk.

Object-oriented features typically have very small overhead compared to procedural code with
equal functionality, so additional complexity introduced is the only downside, in author's
opinion. We believe that these disadvantages are greatly outweighed by powerful features
that object-oriented languages provide (when utilised properly).

It was also determined that
the desired library for Bayesian filtering could benefit from many object-oriented techniques: {\pdf}
and its conditional variant could be easily modelled as classes with abstract methods that
would represent common operation such as evaluation in a given point or drawing random samples.
Classes representing particular {\pdfs} would then subclass abstract base classes and implement
appropriate methods while adding relevant attributes such as border points for uniform
distribution. This would allow for example to create generic form of particle filter
(p.~\pageref{sec:ParticleFilter}) that would accept any conditional {\pdf} as a parameter.
Bayesian filter itself can be abstracted into a class that would provide a method to compute
posterior {\pdf} from prior one taking observation as a parameter.

\subsection{Functional paradigm}
Fundamental idea of the functional programming is that
functions have no side effects --- their result does not change or depend on program state, only
on supplied parameters. A language where each function has mentioned attribute is called
\emph{purely functional} whereas the same adjective is applied to such functions in other
languages. This is often accompanied by a principle that all data are immutable (apart from
basic list-like container type) and that functions are so-called ``first-class citizens''
--- they can be passed to a function and returned. Placing a restriction of no side-effect on
functions allows compiler/interpreter to do various transformations: parallelisation of function
calls whose parameters don't depend on each other's results, skipping function calls where the
result is unused, caching return values for particular parameters.

Among languages specially designed for functional programming are: Haskell, Lisp dialects Scheme
and~Clojure, Erlang. Python supports many functional programming techniques\footnote{e.g. functions
as first-class citizens, closures, list comprehensions}.

While functional programming is popular subject of academic research, its use is much less
widespread compared to procedural and object-oriented paradigms. Additionally, in the author's
opinion, transition to functional programming requires significant change of programmer's
mindset. Combined with the fact that syntax of the mentioned functionally-oriented languages differs
significantly from many popular procedural or object-oriented languages, we believe that it would
be unsuitable decision for a library that aims for wide adoption.

\subsection{Other programming language considerations}

Apart from recently discussed general approaches to programming, we should note a few other
attributes of languages or their implementations that significantly affect software written using
them. The first distinction is based on type system of a language --- we may divide them into
2~major groups:
\begin{description}
	\item[statically typed languages] \hfill \phantomsection \label{desc:StaticTyping} \\
		bind object types to \emph{variables}; vast majority of type-checking is done at
		compile-time. This
		means that each variable can be assigned only values of given type (subject to
		polymorphism); most such languages require that variable (function parameter, object
		attribute) types are properly declared.
	\item[dynamically typed languages] \hfill \phantomsection \label{desc:DynamicTyping} \\
		bind object types to \emph{values}; vast majority of type-checking is done at runtime.
		Programmer can assign and reassign objects of arbitrary types to given variable. Variables
		(and object attributes) are usually declared by assignment.
\end{description}
We consider dynamically typed languages more convenient for programmers --- we're convinced that the
possibility of sensible variable reuse and lack of need to declare variable types lets the
programmer focus more on the actual task, especially during prototyping stage. This convenience however
comes with a~cost: dynamic typing imposes inevitable computing overhead as method calls and
attribute accesses must be resolved at runtime. Additionally, compiling a program written in statically
typed language can reveal many simple programming errors such as calling mistyped methods, even
in unreachable code-paths; this is not the case for dynamically-typed languages and we suggest
compensating this with more thorough test-suite (code coverage tools can greatly help with creating
proper test-suite, see \autoref{sec:PyBayesDocsTests} on page \pageref{sec:PyBayesDocsTests}).

Another related property is interpreted vs. compiled nature; we should emphasize that this property
refers to language \emph{implementation}, not directly to the language itself, e.g. C language
is commonly regarded as compiled one, several C interpreters however exist. We use the term
``language is compiled/interpreted'' to denote that principal implementation of that language is
compiled, respectively interpreted.
\begin{description}
	\item[compiled implementations] \hfill \\
		translate source code directly into machine code suitable for given target processor. Their
		advantage is zero interpreter overhead. Developers are required to install a
		compiler (and perhaps a build system) or an IDE\footnote{Integrated Development Environment}
		used by given project (library) to be able to modify it. Write-build-run-debug cycle is
		usually longer in comparison to interpreted implementations.
	\item[interpreted implementations] \hfill \\
		either directly execute commands in source code or, more frequently, translate source code
		into platform-independent \emph{intermediate representation} which is afterwards executed in
		a \emph{virtual machine}. We may allow the translate and execute steps to be separated so that
		Java and similar languages can be included. Advantages include usually shorter
		write-run-debug cycle that speeds up development and portable distribution options.
		Interpreted languages have been historically associated with considerable processing
		overhead, but \emph{just-in-time compilation}\footnote{interpreter feature that translates
		portions of bytecode into machine code at runtime.} along with \emph{adaptive
		optimisation}\footnote{a technique to use profiling data from recent past (collected perhaps
		when relevant portion of code was run in interpreted mode) to optimise just-in-time compiled
		code.} present in modern interpreters can minimise or even reverse interpreter
		overhead: Paul Buchheit have shown\footnote{\url{http://paulbuchheit.blogspot.com/2007/06/java-is-faster-than-c.html}}
		that second and onward iterations of fractal-generating Java program were actually
		5\%~faster than equivalent C program. We have reproduced the test with following results:
		Java program was 10\% slower (for second and subsequent iterations) than C program and 1600\%
		slower when just-in-time compilation was disabled. Complete test environment along with
		instructions how to reproduce it be found in
		\href{http://github.com/strohel/PyBayes/tree/master/examples/benchmark_c_java}{\nolinkurl{examples/benchmark_c_java}}
		directory in the PyBayes source code repository.
\end{description}
There exists a historic link between statically typed and compiled languages, respectively
dynamically typed and interpreted languages. Java which is itself statically typed and it's major
implementation is interpreted and Erlang's (which is dynamically typed) compiled
HiPE\footnote{The High-Performance Erlang Project: \url{http://www.it.uu.se/research/group/hipe/}}
implementation are some examples of languages that break the rule. We believe that this historic
link is the source of a common misconception that interpreted languages are inherently slow. Our
findings (see also Python/Cython/C benchmark on p. \pageref{sec:CythonPerformace}) indicate that
the source of heavy overhead is likely to be the dynamic type system rather than overhead of modern
just-in-time interpreters. In accordance with these findings, we may conclude that choice of language
implementation type should rather be based on development and distribution convenience than on
expected performance. % TODO: je tato veta dostatecne jasna?

Each programming language may support one or more following function call conventions that determine
how function parameters are passed:
\begin{description}
	\item[call-by-value convention] \hfill \\
		ensures that called function does not change variables passed as parameters from calling
		function by copying them at function call time. This provides clear semantics but incurs
		computational and memory overhead, especially when large data structures are used as
		parameters. As a form of optimisation, some language implementations may employ
		copy-on-write technique so that variables are copied only when they are mutated from within
		called function, thus saving space and time when some parameters are only read from.
	\item[call-by-reference convention] \hfill \\
		hands fully-privileged references to parameters to called function. These references can be
		used to modify or assign to parameters within called function and these changes are visible
		to calling function. This approach minimises function call overhead but may appear confusing
		to a programmer when local variable is changed ``behind her back" unexpectedly. On the other
		hand, call-by-reference allows for programming techniques impossible with call-by-value
		alone (e.g. a function that swaps two values).
	\item[call-by-object (call-by-sharing) convention] \hfill \\
		can be viewed as a compromise between call-by-value and call-by-reference: parameters are
		passed as references that can be used to modify referred objects (unless marked immutable),
		but cannot be used to assign to referred objects (or this assignment is invisible to calling
		function). When an object is marked as immutable,
		passing this object behaves like call-by-value call without copying overhead (in the calling
		function point of view). Java and Python use call-by-object as their sole function calling
		method\footnote{python case: \url{http://effbot.org/zone/call-by-object.htm}} and both mark
		certain elementary types (most prominently numbers and strings) as immutable. C's
		pointer-to-const and C++'s reference-to-const parameters can be viewed as call-by-object
		methods where referred objects are marked as immutable in called function scope.
\end{description}
We suggest that a language that supports at least one of call-by-reference or call-by-object
conventions is used for the desired recursive Bayesian estimation library; while call-by-value-only
languages can be simpler to implement, we are convinced that they impose unnecessary restrictions
on the library design and cause overhead in places where it could be avoided.

Last discussed aspect of programming languages relates to memory management:
\begin{description}
	\item[garbage-collected languages] \hfill \\
		provide memory management in the language itself. This fact considerably simplifies
		programming as programmer doesn't need to reclaim unused memory resources herself. Another advantage
		is that automatic memory management prevents most occurrences of several programming errors:
		memory leaks,\footnote{an error condition when a region of memory is no longer used, but not
		reclaimed.} dangling pointers\footnote{a pointer to an object that has been already destroyed;
		such pointers are highly error-prone.} and double-frees.\footnote{an error condition where a
		single region of memory is reclaimed twice; memory corruption frequently occurs in this
		case.} Two major
		approaches to garbage collection exist and both incur runtime computational or memory
		overhead. \emph{Tracing garbage collector} repeatedly scans program heap\footnote{an area of
		memory used for dynamic memory allocation.} memory for objects
		with no references to them, then reclaims memory used by these objects. Program performance
		may be substantially impacted while tracings garbage collector performs its scan; furthermore
		the moment when garbage collector fires may be unpredictable. \emph{Reference counting}
		memory management works by embedding an attribute, \emph{reference count}, to each object
		that could be allocated on heap and then using this attribute to track number of references
		to given object. When reference count falls to zero, the object can be destroyed. Reference
		counting adds small memory overhead per each object allocated and potentially significant
		computational overhead as reference counts have to be kept up-to-date. However, techniques
		exist that minimise this overhead, for example those mentioned in~\cite{LevPet:06}.
	\item[non garbage-collected languages] \hfill \\
		put the burden of memory management on shoulders of the programmer: she is responsible for
		correctly reclaiming resources when they are no longer in use. The advantages are clear:
		no overhead due to memory management, probably also smaller complexity of language
		implementation. However, as mentioned earlier, languages without automatic memory management
		make certain classes of programmer errors more likely to occur.
\end{description}
In our view, convenience of garbage-collected languages outweighs overhead they bring for a project
like a library for recursive Bayesian estimation targeting wide adoption. We also believe that automatic
memory management can simplify library design and its usage as there is no need to specify who is
responsible for destroying involved objects on the library side and no need to think about it at
the user side.

\section{C++}

C++ is regarded as one of the most popular programming languages today, along with Java and
C;\footnote{TIOBE Programming Community Index for July 2011:
\url{http://www.tiobe.com/index.php/content/paperinfo/tpci/index.html}} it combines properties of
both low-level and high-level languages, sometimes being described as intermediate-level language.
C++ extensively supports both procedural and class-based object-oriented paradigm, forming a
multi-paradigm language; generic programming is implemented by means of \emph{templates}, which
allow classes and functions to operate on arbitrary data types while still being type-safe.
C++ is statically-typed, all major implementations are compiled, supports call-by-value (the
default), call-by-reference and a variant of call-by-object function call conventions. C++ lacks
implicit garbage collection for heap-allocated data --- the programmer must reclaim memory used
by those objects manually; use of \emph{smart pointers}\footnote{a template class that behaves like
a pointer through use of operator overloading but adds additional memory management features such as
reference counting} may although help with this task. C++ is almost 100\% compatible with the C
language in a way that most C programs compile and run fine then compiled as C++ programs. C++ also
makes it easy to use C libraries without a need to recompile them.~\cite{Str:00}

When used as an implementation language for the desired library for recursive Bayesian estimation, we
have identified potential advantages of the C++ language:
\begin{description}
	\item[low overhead] \hfill \\
		C++ was designed to incur minimal overhead possible. In all benchmarks we've seen (e.g. The
		Computer Language Benchmarks Game\footnote{\url{http://shootout.alioth.debian.org/}}), it is
		hard to outperform C++ by a significant margin (Fortran and assembly code would be
		candidates for that).
	\item[widespread] \hfill \\
		C/C++ code forms large part of the software ecosystem. Thanks to that, incredible number of
		both proprietary and free IDEs, debuggers,
		profilers and other related coding tools is available. This fact makes development more
		convenient.
	\item[libraries] \hfill \\
		Thanks to C++ popularity, several high-quality libraries for numerical calculations/computer
		algebra are available, many of them are free software or free to use. These are for example
		C interfaces to BLAS\footnote{Basic Linear Algebra Subprograms: \url{http://www.netlib.org/blas/}}
		and LAPACK\footnote{Linear Algebra PACKage: \url{http://www.netlib.org/lapack/}} (both low-level
		and fixed function), higher-level IT++\footnote{\url{http://itpp.sourceforge.net/}} built
		atop of BLAS/LAPACK or independent template-based library
		Eigen.\footnote{\url{http://eigen.tuxfamily.org/}} Additionally,
		OpenMP\footnote{Open Multi-Processing API and libraries: \url{http://openmp.org/}}
		can be used to parallelise existing algorithms without rewriting them.
\end{description}
However, using C++ would, in our opinion, bring following major drawbacks:
\begin{description}
	\item[diversity] \hfill \\
		While there are many C/C++ libraries for specific tasks (such as data visualisation), it
		may prove difficult in our opinion to combine them freely as there are no \emph{de facto}
		standard data types for e.g. vectors and matrices --- many libraries use their own.
	\item[learning curve] \hfill \\
		C++ takes longer to learn and even when mastered, programmer productivity is subjectively
		lower compared to very high-level languages. We also fear that many members of out intended
		audience are simply unwilling to learn or use C++.
\end{description}
Moreover, discussion about statically-typed, compiled and non-garbage-collected languages from
previous section also apply. Due to this, we have decided not to use C++ if an alternative with
reasonable overhead is found.

Several object-oriented C++ libraries for recursive Bayesian estimation exist:
Bayes++\footnote{\url{http://bayesclasses.sourceforge.net/}}, BDM~\cite{BDM} and BFL~\cite{BFL}.
BDM library is later used to compare performance of Cython, C++ and MATLAB implementations of the
Kalman filter, see \autoref{sec:PyBayesPerformance} on page \pageref{sec:PyBayesPerformance}.

\section{MATLAB language}

MATLAB language is a very high-level language used exclusively by the
MATLAB\footnote{\url{http://www.mathworks.com/products/matlab/}} environment, a~proprietary platform
developed by MathWorks.\footnote{\url{http://www.mathworks.com/}} MATLAB language extensively
supports procedural programming paradigm and since version 7.6 (R2008a) class-based object oriented
paradigm is also fully supported.\footnote{\url{http://www.mathworks.com/products/matlab/whatsnew.html}}
MATLAB language is dynamically-typed, interpreted language with automatic memory management.

MATLAB language possesses, in our belief, following favourable attributes when used to implement the
desired library for Bayesian filtering:
\begin{description}
	\item[popularity among academia] \hfill \\
		While MATLAB language is not as widespread as C++ on the global scale, it is very popular in
		scientific community, our intended audience.
	\item[performance] \hfill \\
		MATLAB language is very well optimised for numerical computing.
	\item[wide range of extensions] \hfill \\
		High number of well integrated extension modules (toolboxes) is bundled with MATLAB or
		available from third parties. This makes associated tasks such as data visualisation
		particularly straightforward.
	\item[rapid development] \hfill \\
		Being a very high-level language, we expect programmer productivity in the MATLAB language being
		fairly high. MATLAB environment is itself a good IDE and its interactive shell fosters rapid
		prototyping.
\end{description}
Following disadvantages of the MATLAB language were identified:
\begin{description}
	\item[vendor lock-in] \hfill \\
		MATLAB is commercial software; free alternatives such as GNU
		Octave\footnote{\url{http://www.gnu.org/software/octave/}},
		Scilab\footnote{\url{http://www.scilab.org/}} or
		FreeMat\footnote{\url{http://freemat.sourceforge.net/}} exist, however all of them provide
		only limited compatibility with the MATLAB language. Developing for a non-standard proprietary
		platform always imposes risks of the vendor changing license or pricing policy etc.
	\item[problematic object model] \hfill \\
		We have identified in \autoref{sec:OOP} that object-oriented approach is important for
		a~well-designed and usable library for Bayesian filtering. Nonetheless MATLAB's
		implementation of object-oriented programming is viewed as problematic by many, including us. For
		example, function call parameter passing convention is determined by the object class/data
		type --- MATLAB distinguishes \emph{value classes} that have call-by-value semantics and
		\emph{handle classes} that have call-by-object semantics.\footnote{call-by-object semantics
		tested in version 7.11 (R2010b).} The resulting effect is that calling
		identical function with otherwise equivalent value and handle classes can yield very
		different behaviour.
	\item[hard-coded call-by-value semantics] \hfill \\
		2D array, a very central data-type of the MATLAB language, has call-by-value function call
		convention hard-coded; this results in potentially substantial function call overhead.
		Although current MATLAB versions try to minimise copying by employing copy-on-write
		technique\footnote{\url{http://blogs.mathworks.com/loren/2006/05/10/memory-management-for-functions-and-variables/}}
		or performing some operations in-place,\footnote{\url{http://blogs.mathworks.com/loren/2007/03/22/in-place-operations-on-data/}}
		our tests have shown that even combining these techniques doesn't eliminate unnecessary
		copying overhead which we believe is the main source of grave performance regression of
		object-oriented code with regards to imperative code; see \autoref{sec:PyBayesPerformance}
		on page \pageref{sec:PyBayesPerformance}.
%	\item[hard to integrate] \hfill \\ % TODO: matlab call C code, c code call matlab; matlab compiler
\end{description}
We consider presented drawbacks significant and therefore decided not to use the MATLAB language for
the desired Bayesian filtering library. BDM library~\cite{BDM} contains both object oriented and
imperative implementation of the Kalman filter in the MATLAB language; these are compared with our
implementation in \autoref{sec:PyBayesPerformance}.

\section{Python}

Python\footnote{\url{http://www.python.org/}} is a very high level programming language designed for
outstanding code readability and high programmer productivity actively developed by the
Python Software Foundation.\footnote{\url{http://www.python.org/psf/}} Python extensively supports
procedural and class-based object-oriented programming paradigms and some features of the functional
programming. Python is dynamically-typed language with automatic memory management that exclusively employs
call-by-object function call parameter passing convention; elementary numeric types, strings and
tuples are immutable\footnote{\url{http://docs.python.org/reference/datamodel.html}} so that this
approach doesn't become inconvenient.

Principal Python
implementation, CPython, is written in C, is cross-platform and of interpreted type: it translates
Python code into bytecode which is subsequently executed in a virtual machine. Many alternative
implementations are available, to name a few: Jython\footnote{\url{http://www.jython.org/}} that
translates Python code into Java bytecode (itself written in Java),
IronPython\footnote{\url{http://ironpython.net/}} itself implemented on top of the .NET Framework,
just-in-time compiling PyPy\footnote{\url{http://pypy.org/}} written in Python itself or
Cython which is described in greater detail in the next section. All the
mentioned implementations qualify as free/open-source software.

Python language is bundled with a comprehensive standard library so that writing new projects is quick
from the beginning. Two major Python versions exists: Python 2, considered legacy and receiving only
bugfix updates, and Python 3, actively developed and endorsed version that brings a few incompatible
changes to the language syntax and to the standard library. Porting Python 2 code to version 3 is
however usually straightforward and can be automated to a great extent with tools bundled with
Python 3.

In our belief, Python shows following favourable attributes when used for the desired Bayesian
filtering library:
\begin{description}
	\item[development convenience, readability, rapid prototyping] \hfill \\
		Python developers claim that Python in an easy to learn, powerful programming language and
		our experience confirms their claims. Python code is easy to prototype, understand and
		modify in our opinion; prototyping is  with bundled interactive Python shell.
		While all these statements are subjective, they are shared among
		many.\footnote{\url{http://python.org/about/quotes/}} For example a statement \verb|x <= y <= z|
		has its mathematical meaning, which is unusual for programming languages.
	\item[NumPy, SciPy, Matplotlib] \hfill \\
		NumPy\footnote{\url{http://www.numpy.org/}} is the de facto standard Python library for
		numeric computing; NumPy provides N-dimensional array type that is massively supported in
		very high number of projects. Parts of NumPy are written in C and Cython for speed.
		SciPy\footnote{\url{http://www.scipy.org/}} extends NumPy with more numerical routines.
		Matplotlib\footnote{\url{http://matplotlib.sourceforge.net/}} is powerful plotting library
		that natively supports SVG output. Combining these three and Python gives a very vital
		MATLAB alternative.
	\item[interoperability with C] \hfill \\
		CPython makes it possible to write modules\footnote{module in python sense is a code unit with
		its own namespace, normally each module corresponds to a .py file.} in
		C\footnote{\url{http://docs.python.org/extending/index.html}} (that are then called \emph{extension modules}).
		Cython makes it easy and convenient to write extension modules. Sadly, alternative
		implementations PyPy, Jython and IronPython don't currently fully support extension modules
		and are therefore ruled-out for our purposes because they in turn don't support
		NumPy.\footnote{PyPy and IronPython are nonetheless interesting for future consideration as
		both have NumPy support actively worked on.}
	\item[interoperability with MATLAB] \hfill \\
		SciPy contains procedures to load and save data in MATLAB .mat format; Matplotlib includes
		programming interface that resembles MATLAB's plotting procedures.
\end{description}
On the other hand, a few downsides exist:
\begin{description}
	\item[overhead] \hfill \\
		CPython implementation shows significant computational overhead, especially for numerical
		computations; CPython doesn't currently utilise any form of just-in-time compiling.
		NumPy is often used to trade off computational overhead for memory overhead:
		The Computer Language Benchmarks Game\footnote{\url{http://shootout.alioth.debian.org/}}
		contains an~example where a program heavily using NumPy is 20\(\times\) faster but consumes
		12\(\times\) more memory than a program that performed the same task and used solely the
		Python standard library. We have reproduced the benchmark with similar results; mentioned
		programs can me found in the
		\href{http://github.com/strohel/PyBayes/tree/master/examples/benchmark_py_numpy}{\nolinkurl{examples/benchmark_py_numpy}}
		directory in the PyBayes source code repository. We still consider this workaround suboptimal.
	\item[peculiar parallelisation] \hfill \\
		While Python natively supports threads and they are useful for tasks such as background I/O
		operations, Python threads don't scale on multiprocessor systems for CPU-bound processing;
		such code often runs at single-processor speed (when run in CPython). The reason behind that
		is that CPython employs a global-interpreter-lock to to assure that only one thread executes
		Python bytecode at a time.\footnote{\url{http://docs.python.org/glossary.html}} This
		restriction can be worked around by using multiple python interpreters that communicate with
		each other; Python module \emph{multiprocessing} makes it almost as convenient as using
		threads.
\end{description}
Python compares favourably to other implementation environments presented before in our view; sole
major obstacle being excessive overhead of the CPython interpreter. It is discussed how this issue
can be solved using Cython in the next section.

We haven't found any Python library for recursive Bayesian estimation that would fulfil the
requirements presented at the beginning of this chapter.

\section{Cython}

TODO

general info etc... extension types, building, ease of interfacing C (and F) code, .pxd files,
NumPy support

[citations:\cite{BehBraSel:09,Sel:09,BehBraCitDalSelSmi:11}]

\subsection{Gradual Optimisation}

how can optimisaion be approached (gradually) and why this approach is superior

[see c\_cy\_py...]

\subsection{Parallelisation}

integrate\_python\_cython patched with OpenMP (13x speedup in 16-core system)

prange CEP -- implemented!

[see c\_cy\_py...]

\subsection{Pure Python mode}

About it and why it should be used in a hypothetical bayesian python library

\subsection{Limitations}

2 types:

	not-supported code (few cases, but bad, ongoing work)

	not-optimised code (much more work needed, but not hard to fix in most cases)

		- exception handling (functions returning void etc)

		- limitations of pure python mode in regards to traditional .pyx files

\subsection{Performance comparison with C and Python} \label{sec:CythonPerformace}

benchmark\_c\_cy\_py

